{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "uasra6ixonkgp2gxfeyu",
   "authorId": "927285349650",
   "authorName": "NSVERCHEK",
   "authorEmail": "nicholas.sverchek@snowflake.com",
   "sessionId": "aace0ce3-8fbb-4454-8e73-d09e47489cbe",
   "lastEditTime": 1749778306514
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ff8eb8-9131-4fa2-9805-62ffc561ce5e",
   "metadata": {
    "collapsed": false,
    "name": "intro_md",
    "resultHeight": 516
   },
   "source": "# Main Data Setup\n\nIn this Notebook on **Container Runtime**, we will first **prepare all the unstructured data** needed before we can run the Streamlit App. Once this data is processed, the chatbot will have a rich knowledge base to start from that's all stored within the [Cortex Search](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview) service, a fully managed indexing and retrieval service. Cortex Search will then be used for RAG.\n\nThere are two types of data we're dealing with in this solution:\n- **Audio files**: previously recorded calls between a call center agent and a member\n- **PDF files**: FAQ docs for call center agents to help answer member inquiries\n\n**Why is Container Runtime needed?**\\\nSince we have audio files, we will need to install OpenAI Whisper in order to transcribe those files into text. OpenAI Whisper requires `ffmpeg` to be installed, which cannot be installed in Warehouse compute. We will also use GPU compute here, which makes it much faster to transcribe these files.\n"
  },
  {
   "cell_type": "markdown",
   "id": "a17e69a9-8199-4718-ab10-a1da6c2e7007",
   "metadata": {
    "collapsed": false,
    "name": "cortex_search_md",
    "resultHeight": 232
   },
   "source": [
    "### Cortex Search \n",
    "\n",
    "Cortex Search gets you up and running with a hybrid (vector and keyword) search engine on your text data in minutes, without having to worry about embedding, infrastructure maintenance, search quality parameter tuning, or ongoing index refreshes.\n",
    "\n",
    "It powers a broad array of search experiences for Snowflake users including [Retrieval Augmented Generation (RAG)](Retrieval Augmented Generation (RAG)) applications leveraging Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd3394-7af9-4c7b-8e47-9442d446cefb",
   "metadata": {
    "collapsed": false,
    "name": "cortex_rag_md",
    "resultHeight": 228
   },
   "source": [
    "### Cortex Search for RAG\n",
    "\n",
    "Retrieval augmented generation (RAG) is a technique for retrieving data from a knowledge base to enhance the generated response of a large language model.\n",
    "\n",
    "#### Using Cortex Search for RAG in Snowflake\n",
    "Cortex Search is the retrieval engine that provides the Large Language Model with the context it needs to return answers that are grounded in your most up-to-date proprietary data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f631c1c-a8a6-40b0-9de9-1525e26308c3",
   "metadata": {
    "collapsed": false,
    "name": "start_md",
    "resultHeight": 60
   },
   "source": [
    "## Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86573c-5555-42b1-acf2-d31296d7075f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "imports",
    "resultHeight": 38
   },
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom snowflake.core import Root\nfrom snowflake.snowpark.context import get_active_session\nimport glob\nimport numpy as np\n\nsession = get_active_session()\nroot = Root(session)\n\n# Add a query tag to the session. This helps with debugging and performance monitoring.\nsession.query_tag = {\"origin\":\"sf_sit\", \n                     \"name\":\"payer_call_center_assistant_v2\", \n                     \"version\":{\"major\":1, \"minor\":0},\n                     \"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\"}}\n\n# Set session context \nsession.use_role(\"SYSADMIN\")\n\n# Print the current role, warehouse, and database/schema\nprint(f\"role: {session.get_current_role()} | WH: {session.get_current_warehouse()} | DB.SCHEMA: {session.get_fully_qualified_current_schema()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "27ebd2bf-f46b-40bd-ba3b-979bb0dca196",
   "metadata": {
    "collapsed": false,
    "name": "part1_transcribe_audio_md",
    "resultHeight": 195
   },
   "source": [
    "## 1. Transcribe Audio Files\n",
    "\n",
    "For this portion, we will download OpenAI's [whisper](https://github.com/openai/whisper) model (a pretrained model), and use it for inference. In this case, we're just passing audio files to the model to output transcriptions. \n",
    "\n",
    "In order to install `whisper`, we'll need `ffmpeg`, and there's a provided shell script within the Notebook files repo to get it installed since it's not a Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b348e-42d4-4a2b-b52b-27c887faf7fb",
   "metadata": {
    "collapsed": false,
    "name": "install_ffmpeg_md",
    "resultHeight": 42
   },
   "source": [
    "First, install `ffmpeg` by running the setup script provided in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca3cb1-bcde-42a4-adcd-83e5b2d95c1d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "install_ffmpeg",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Run this script to install ffmpeg\n",
    "!sh ffmpeg_install.sh > out.log 2> err.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88231e-f173-406a-8dec-bfe0aaf901ef",
   "metadata": {
    "language": "python",
    "name": "optional_logs",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment if you want to see the installation logs\n",
    "#!cat out.log\n",
    "#!cat err.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596db8d3-4129-4edb-b9f8-9f9c436c8838",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "check_installation",
    "resultHeight": 38
   },
   "outputs": [],
   "source": [
    "# Make sure it got installed\n",
    "!which ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce92033-144c-48e2-b79d-21498cff55f6",
   "metadata": {
    "collapsed": false,
    "name": "install_whisper_md",
    "resultHeight": 41
   },
   "source": [
    "Now, we install OpenAI's Whisper model to transcribe the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e08ae8-1cd6-481a-bed1-0a690c940c14",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "install_whisper",
    "resultHeight": 66
   },
   "outputs": [],
   "source": [
    "# Install whisper\n",
    "\n",
    "# Note: --quiet suppresses the output. \n",
    "#       You can remove it if you'd like to \n",
    "#       see all the installation messages.\n",
    "\n",
    "!pip install openai-whisper --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac2fb7-5017-4c5e-a25d-c1fb3fc9e39c",
   "metadata": {
    "collapsed": false,
    "name": "load_model_md",
    "resultHeight": 41
   },
   "source": [
    "Now, we can load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda741a-b6d0-4f97-87c7-d43237048411",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "load_whisper",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Load whisper model\n",
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d22125-488b-457f-b987-11d316766d38",
   "metadata": {
    "collapsed": false,
    "name": "download_files_md",
    "resultHeight": 41
   },
   "source": [
    "Our audio files live in a stage, so we'll download them into this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96734c-391c-48ff-984c-fe52dc0e4f03",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "download_audio_files",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Download all files from stage\nf = session.file.get('@RAW_DATA_TX/CALL_RECORDINGS/', 'call_recordings/')"
  },
  {
   "cell_type": "markdown",
   "id": "ecfd274f-552f-4e79-8803-229434812d53",
   "metadata": {
    "collapsed": false,
    "name": "helper_func_md",
    "resultHeight": 67
   },
   "source": [
    "We'll create a helper function to transcribe the audio, which includes a few audio processing steps before it's ready to pass to the model to decode the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ee8b9-0a35-437a-a923-11bdcc9266ff",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_transcription_function",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Create function to transcribe all audio\ndef transcribe_audio_bkup(audio_file_name):\n    '''\n        Transcribe audio files\n    '''\n    # load audio and pad/trim it to fit 30 seconds\n    print(f\"Transcribing: {audio_file_name}\")\n    audio = whisper.load_audio(audio_file_name)\n    #audio = whisper.pad_or_trim(audio)\n    \n    # make log-Mel spectrogram and move to the same device as the model\n    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n    \n    # detect the spoken language\n    _, probs = model.detect_language(mel)\n    language = max(probs, key=probs.get)\n    #print(f\"Detected language: {max(probs, key=probs.get)}\")\n    print(f\"Detected language: {language}\")\n   \n    \n    # decode the audio\n    options = whisper.DecodingOptions()\n    result = whisper.decode(model, mel, options)\n    print(\"Text\", result.text)\n    \n    # return the recognized text\n    return result.text, language"
  },
  {
   "cell_type": "markdown",
   "id": "d0472d21-229d-4f16-971d-73bf12a4f37e",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "The above example truncated the audio files to 30 seconds. Needed to find a way to transcribe the entire conversation. Below I check to see if the file is > 30 seconds, and if it is, break it up into chunks, append each chunk and then save the entire transcribed converstation. \n\n- SAMPLE_RATE = 16000\n- CHUNK_LENGTH = 30\n- N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk"
  },
  {
   "cell_type": "code",
   "id": "542c07a0-2089-458b-aedb-d64bfcf68dd0",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Create function to transcribe all audio\ndef transcribe_audio(audio_file_name):\n\n    CHUNK_LIM = 480000\n    audios = []\n\n    \n    '''\n        Transcribe audio files\n    '''\n    # load audio and pad/trim it to fit 30 seconds\n    print(f\"Transcribing: {audio_file_name}\")\n    audio = whisper.load_audio(audio_file_name)\n\n\n    if len(audio) <= CHUNK_LIM:\n        audios.append(audio)\n\n    else:\n        for i in range(0, len(audio), CHUNK_LIM):\n            chunk = audio[i:i + CHUNK_LIM]\n            chunk_index = len(chunk)\n            if chunk_index < CHUNK_LIM:\n                padding = [0] * (CHUNK_LIM - chunk_index)\n                array1 = np.array(chunk)\n                array2 = np.array(padding)\n                concat =  np.concatenate((array1, array2))\n                chunk = concat.astype(np.float32)\n            audios.append(chunk)\n\n    results = \"\"\n    language = \"\"\n\n    for chunk in audios:\n        \n        # make log-Mel spectrogram and move to the same device as the model\n        mel = whisper.log_mel_spectrogram(chunk).to(model.device)\n        _, probs = model.detect_language(mel)\n        language = max(probs, key=probs.get)\n        #print(f\"Detected language: {max(probs, key=probs.get)}\")\n        print(f\"Detected language: {language}\")\n        \n        # decode the audio\n        #options = whisper.DecodingOptions(fp16=False)\n        options = whisper.DecodingOptions()\n        result = whisper.decode(model, mel, options)\n        results += result.text\n    \n\n    return results, language",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d722b8e-f3f2-4c2b-8204-57d62d393186",
   "metadata": {
    "collapsed": false,
    "name": "apply_function_md",
    "resultHeight": 41
   },
   "source": [
    "We'll apply this function to all our audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defe869-4e8e-48ca-8287-2452e7c7ce63",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "process_audtio_files",
    "resultHeight": 1337,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Process all audio files and store in a list\naudio_files = glob.glob('call_recordings/*.mp3')\n\nall_transcribed = []\n\nfor f in audio_files:\n    text, language = transcribe_audio(f)\n    #all_transcribed.append((f, transcribe_audio(f)))\n    all_transcribed.append((f, text, language))"
  },
  {
   "cell_type": "markdown",
   "id": "fc7e9d4b-e1ef-4793-87a5-40b68442a4de",
   "metadata": {
    "collapsed": false,
    "name": "check_transcriptions_md",
    "resultHeight": 41
   },
   "source": [
    "Let's take a look at a few of the transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b1367-2b4b-40d5-a238-ac7dc85806bd",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "check_transcriptions",
    "resultHeight": 54,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Look at a few of the transcriptions\nall_transcribed[0:10]"
  },
  {
   "cell_type": "markdown",
   "id": "dfff721a-0653-4fd5-8a7b-2ba5e96a0d43",
   "metadata": {
    "collapsed": false,
    "name": "write_results_md",
    "resultHeight": 41
   },
   "source": [
    "Now we'll store all the results in a Snowpark DF and write it to a Snowflake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b869b4-d675-441a-bded-e65f9b4965e9",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "create_df_for_results",
    "resultHeight": 295
   },
   "outputs": [],
   "source": "# Create a Snowpark DataFrame from the transcriptions\ndf = session.create_dataframe(all_transcribed, schema=[\"AUDIO_FILE_NAME\", \"TRANSCRIPT\", \"LANGUAGE\"])\ndf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878bc98-e7b8-47ff-831f-3602ef679c6c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "save_results_to_table",
    "resultHeight": 239
   },
   "outputs": [],
   "source": "# Save results as a Snowflake Table\n#df.write.mode(\"overwrite\").save_as_table(\"CALL_RECORDINGS_TRANSCRIPT_TABLE\")\ndf.write.mode(\"overwrite\").save_as_table(\"CALL_RECORDINGS_TRANSCRIPT_TABLE_ORIG\")"
  },
  {
   "cell_type": "code",
   "id": "1f1d31c4-d268-41da-932f-4504dac7a926",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "SELECT * FROM CALL_RECORDINGS_TRANSCRIPT_TABLE_ORIG",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a80e0164-285f-4c3c-9de1-e9560a2a80bf",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "Translate the table"
  },
  {
   "cell_type": "code",
   "id": "2b4c0a7c-177b-4836-b131-020b880468fa",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE CALL_RECORDINGS_TRANSCRIPT_TABLE_TX AS\nSELECT \n    AUDIO_FILE_NAME,\n    TRANSCRIPT AS ORIG_TRANSCRIPT,\n    LANGUAGE,\n    build_scoped_file_url(@RAW_DATA_TX, AUDIO_FILE_NAME) as scoped_file_url,\n    CASE \n        WHEN LANGUAGE != 'en' THEN \n            SNOWFLAKE.CORTEX.TRANSLATE(TRANSCRIPT, '', 'en')  -- Ensure external function syntax is correct\n        ELSE \n            TRANSCRIPT  -- Return the original value if it's in English\n    END AS TRANSCRIPT\nFROM CALL_RECORDINGS_TRANSCRIPT_TABLE_ORIG;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aec9c663-2975-4514-9c89-4e11899ca0bc",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "SELECT * FROM CALL_RECORDINGS_TRANSCRIPT_TABLE_TX",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03f16680-3449-45cf-93ee-e28f874a2f44",
   "metadata": {
    "collapsed": false,
    "name": "audio_cortex_search_md",
    "resultHeight": 41
   },
   "source": [
    "Finally, we create a Cortex Search service on top of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635b441-f94d-4a2d-a425-137059697bec",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "audio_cortex_search",
    "resultHeight": 112
   },
   "outputs": [],
   "source": "-- Create Cortex Search Service\n--use 2.0 lanague model  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\nCREATE OR REPLACE CORTEX SEARCH SERVICE CALL_CENTER_RECORDING_SEARCH_TX2\nON CHUNK\nWAREHOUSE = PAYERS_CC_WH\nEMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\nTARGET_LAG = '1 Day'\nAS\n(\n    SELECT\n        TRANSCRIPT AS CHUNK,\n        AUDIO_FILE_NAME AS RELATIVE_PATH,\n        scoped_file_url\n    FROM\n        CALL_RECORDINGS_TRANSCRIPT_TABLE_TX\n        \n)"
  },
  {
   "cell_type": "code",
   "id": "ccc11be9-3b1c-411f-963f-fd5f6b052e59",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e0c0ff8-0d15-4798-8558-6ea05f863302",
   "metadata": {
    "collapsed": false,
    "name": "test_audio_cortex_md",
    "resultHeight": 41
   },
   "source": [
    "We can quickly test the service to make sure it was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e9f88-32a7-4897-a2f7-08571377bccc",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "test_audio_cortex_search",
    "resultHeight": 825
   },
   "outputs": [],
   "source": "# Test out the service\n\nresponse = (root.databases[session.get_current_database()]\n                 .schemas[session.get_current_schema()]\n                 .cortex_search_services[\"CALL_CENTER_RECORDING_SEARCH_TX2\"]\n                 .search(\n                            'Santa',\n                              ['CHUNK',\n                               'RELATIVE_PATH'],\n                         limit=8\n                         )\n    )\n\nresults = response.results\nresults"
  },
  {
   "cell_type": "markdown",
   "id": "72ba9fa6-5f78-422c-9406-6c18b995c0f8",
   "metadata": {
    "collapsed": false,
    "name": "part2_process_pdfs_md",
    "resultHeight": 127
   },
   "source": "## 2. Process PDF Files\n\nFor this portion, we'll use Snowflake's native [PARSE_DOCUMENT](https://docs.snowflake.com/en/sql-reference/functions/parse_document-snowflake-cortex) and [SPLIT_TEXT_RECURSIVE_CHARACTER](https://docs.snowflake.com/en/sql-reference/functions/split_text_recursive_character-snowflake-cortex) to read and chunk a PDF."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace41417-a3ff-4692-97bd-ca90ab6ebaf8",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "process_pdfs",
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TEMPORARY TABLE FAQ_DOCS_TEMP AS\n    SELECT\n        RELATIVE_PATH, \n        SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n            to_variant(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n                @RAW_DATA_2,\n                RELATIVE_PATH,\n                {'mode': 'layout'}\n            )):content, 'markdown', 384000, 300) as chunks\nfrom DIRECTORY(@RAW_DATA_2)\nwhere RELATIVE_PATH ilike '%FAQ%';\n\nCREATE OR REPLACE TABLE FAQ_DOCS_CHUNKS_TABLE AS\nSELECT RELATIVE_PATH, c.value::string as CHUNK \nFROM \n    FAQ_DOCS_TEMP f, \n    LATERAL FLATTEN(INPUT => f.chunks) c;"
  },
  {
   "cell_type": "markdown",
   "id": "bbe395dc-5e8b-4d01-b6ac-56a3040ff09f",
   "metadata": {
    "collapsed": false,
    "name": "check_chunks_md",
    "resultHeight": 41
   },
   "source": [
    "Let's make sure the files were properly read and chunked now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3309947-6af7-4645-8c46-00ec2c1b618d",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "check_chunks",
    "resultHeight": 322
   },
   "outputs": [],
   "source": [
    "-- Make sure files were properly read and chunked\n",
    "SELECT * FROM FAQ_DOCS_CHUNKS_TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f21d-3885-41b4-acf6-3cd2b654a3fa",
   "metadata": {
    "collapsed": false,
    "name": "pdf_cortex_search_md",
    "resultHeight": 41
   },
   "source": [
    "Finally, we create a Cortex Search service on top of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ce5bc-ca83-4612-85f4-62c388f75c49",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "pdf_cortex_search",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE CALL_CENTER_FAQ_SEARCH\n",
    "ON CHUNK\n",
    "WAREHOUSE = PAYERS_CC_WH\n",
    "TARGET_LAG = '1 Day'\n",
    "AS\n",
    "(\n",
    "    SELECT\n",
    "        CHUNK,\n",
    "        RELATIVE_PATH\n",
    "    FROM\n",
    "        FAQ_DOCS_CHUNKS_TABLE\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eedecbd-2d51-4c76-9b3b-16d94360e1f2",
   "metadata": {
    "collapsed": false,
    "name": "test_pdf_cortex_md",
    "resultHeight": 41
   },
   "source": [
    "We can quickly test the service to make sure it was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1109067-e9c8-4768-933b-07f2dcb54b46",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "test_pdf_cortex_search",
    "resultHeight": 1205
   },
   "outputs": [],
   "source": [
    "# Test out the service\n",
    "\n",
    "response = (root.databases[session.get_current_database()]\n",
    "                 .schemas[session.get_current_schema()]\n",
    "                 .cortex_search_services[\"CALL_CENTER_FAQ_SEARCH\"]\n",
    "                 .search(\n",
    "                     'Were there any revisions to COVID related coverages?',\n",
    "                     ['CHUNK','RELATIVE_PATH'], limit=3\n",
    "                        )\n",
    "           )\n",
    "\n",
    "results = response.results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66c8d8-545b-4cd6-98fc-0114c462467d",
   "metadata": {
    "collapsed": false,
    "name": "conclusion1",
    "resultHeight": 46
   },
   "source": [
    "### :tada: All the unstructured data is now processed and ready to be used by the Streamlit App!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9b6f6-4037-4e9b-b4df-cf625b63f858",
   "metadata": {
    "collapsed": false,
    "name": "intro_caller_intent_md",
    "resultHeight": 435
   },
   "source": [
    "## Caller Intent Prediction\n",
    "\n",
    "Now, we will predict the intent of a caller using historical data.\n",
    "\n",
    "We will be using the [Classification function from the suite of Snowflake's ML Functions](https://docs.snowflake.com/en/user-guide/ml-functions/classification) to build our model.\n",
    "> - _Classification involves creating a classification model object, passing in a reference to the training data. The model is fitted to the provided training data. You then use the resulting schema-level classification model object to classify new data points and to understand the model’s accuracy through its evaluation APIs._\n",
    "> - _The classification function is powered by a gradient boosting machine. For binary classification, the model is trained using an area-under-the-curve loss function. For multi-class classification, the model is trained using a logistic loss function._\n",
    "\n",
    "The `Caller Intent Prediction Model` will be trained using the relationship between call reasons and current member properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09893baa-7398-4c2e-a6a6-af894e253bf5",
   "metadata": {
    "collapsed": false,
    "name": "classification_view_md",
    "resultHeight": 41
   },
   "source": [
    "We will first create a view including Member attributes that we want to train the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfce0b-b0c5-4dab-9d3e-9646c7641f3b",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "classification_view",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE VIEW CALLER_INTENT_CLASSIFICATION_VIEW AS\n",
    "    SELECT \n",
    "        RECENT_ENROLLMENT_EVENT_IND,\n",
    "        PCP_CHANGE_IND, \n",
    "        ACTIVE_CM_PROGRAM_IND,\n",
    "        CHRONIC_CONDITION_IND,\n",
    "        ACTIVE_GRIEVANCE_IND,\n",
    "        ACTIVE_CLAIM_IND,\n",
    "        POTENTIAL_CALLER_INTENT_CATEGORY\n",
    "FROM CALLER_INTENT_TRAIN_DATASET;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e1476-a301-40ce-a385-6f6731329631",
   "metadata": {
    "collapsed": false,
    "name": "classification_func_md",
    "resultHeight": 41
   },
   "source": [
    "Now, we will create an [ML Classification](https://docs.snowflake.com/en/user-guide/ml-functions/classification) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e819b-f16d-4144-ae40-4be1f17a9cb2",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "classification_func",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE SNOWFLAKE.ML.CLASSIFICATION CALLER_INTENT(\n",
    "    INPUT_DATA => SYSTEM$REFERENCE('view', 'CALLER_INTENT_CLASSIFICATION_VIEW'),\n",
    "    TARGET_COLNAME => 'POTENTIAL_CALLER_INTENT_CATEGORY'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69945fc-3812-4dca-939e-b7e6de4fdd4f",
   "metadata": {
    "collapsed": false,
    "name": "preds_md",
    "resultHeight": 41
   },
   "source": [
    "We will apply our ML model's prediction function on our prediction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c2a21-e2b4-4056-bc3d-2111d77077bc",
   "metadata": {
    "language": "sql",
    "name": "preds_table",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE CALLER_INTENT_PREDICTIONS AS\n",
    "SELECT *, CALLER_INTENT!PREDICT(\n",
    "    INPUT_DATA => {*})\n",
    "    as predictions from CALLER_INTENT_PREDICT_DATASET;\n",
    "SELECT * FROM CALLER_INTENT_PREDICTIONS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ffe63-8193-44ec-8d8c-67c0d32bb810",
   "metadata": {
    "collapsed": false,
    "name": "final_view_md",
    "resultHeight": 41
   },
   "source": [
    "We now create a view to join our member data with our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f400c-70ee-44cc-b5ce-2a416d4e57da",
   "metadata": {
    "language": "sql",
    "name": "final_view",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE VIEW CALL_CENTER_MEMBER_DENORMALIZED_WITH_INTENT\n",
    "AS\n",
    "SELECT\n",
    "    A.*,\n",
    "    B.predictions:class::STRING AS POTENTIAL_CALLER_INTENT\n",
    "FROM \n",
    "CALL_CENTER_MEMBER_DENORMALIZED A\n",
    "LEFT OUTER JOIN \n",
    "CALLER_INTENT_PREDICTIONS B\n",
    "ON A.MEMBER_ID = B.MEMBER_ID;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b24a70-6632-4bef-9b92-9075ae50c634",
   "metadata": {
    "collapsed": false,
    "name": "update_members_md",
    "resultHeight": 41
   },
   "source": [
    "We will now cleanse some data for our sample members who will be part of the Streamlit demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afb405-15bf-47f5-bc19-1aee1e1ea588",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "update_member1",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "UPDATE CALL_RECORDINGS_TRANSCRIPT_TABLE \n",
    "SET TRANSCRIPT = replace(TRANSCRIPT, '159-568-6880','159568380')\n",
    "WHERE TRANSCRIPT ILIKE '%Tracy%Smith%';;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c5eaa-911f-4681-a883-d73f72477fa3",
   "metadata": {
    "language": "sql",
    "name": "update_member2",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "UPDATE CALL_CENTER_MEMBER_DENORMALIZED\n",
    "SET CLAIM_PROVIDER = 'Howe Group'\n",
    "WHERE GRIEVANCE_STATUS = 'Pending'\n",
    "AND GRIEVANCE_TYPE = 'Inadequate Care'\n",
    "AND CLAIM_PROVIDER in ('Thornton Group','Kent Group','Perez-Martinez');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347eb44-fb8c-4cc0-8682-c999484b1f20",
   "metadata": {
    "collapsed": false,
    "name": "conclusion2",
    "resultHeight": 69
   },
   "source": [
    "### :tada: We now have `Caller Intent` predictions, which are ready to be used by the Streamlit App!"
   ]
  }
 ]
}